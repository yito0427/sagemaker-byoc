{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ローカルのnotebook開発から、学習ジョブ移行までを実施する"
   ]
  },
  {
   "source": [
    "# 1. ローカルノートブックで学習を行う\n",
    "・sklearnの乳がんデータを用いる（分類問題）\n",
    "・LightGBMを使う"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1-1.データの保存"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "\n",
    "df = pd.DataFrame(np.append(data.data, data.target.reshape(-1,1), axis=1), columns=np.append(data.feature_names,\n",
    " 'target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(569, 31)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890     0.0  \n",
       "1          0.2750                  0.08902     0.0  \n",
       "2          0.3613                  0.08758     0.0  \n",
       "3          0.6638                  0.17300     0.0  \n",
       "4          0.2364                  0.07678     0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../input/breast_cancer.csv', index=False)"
   ]
  },
  {
   "source": [
    "## 1-2.データ分割"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x, y = dataset.data, dataset.target\n",
    "x, y = df[data.feature_names], df['target']\n",
    "# データセットを学習用とテスト用に分割する\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y,\n",
    "                                                    test_size=0.166,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)\n",
    "# さらに学習用データを学習用とvalid用に分割する\n",
    "tr_x, va_x, tr_y, va_y = train_test_split(train_x, train_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(379, 30)\n(379,)\n(95, 30)\n(95,)\n(95, 30)\n(95,)\n"
     ]
    }
   ],
   "source": [
    "print(tr_x.shape)\n",
    "print(tr_y.shape)\n",
    "print(va_x.shape)\n",
    "print(va_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([tr_x,tr_y], axis=1).to_csv('../opt/ml/input/data/train/train.csv', index=False)\n",
    "pd.concat([va_x,va_y], axis=1).to_csv('../opt/ml/input/data/valid/valid.csv', index=False)\n",
    "pd.concat([test_x,test_y], axis=1).to_csv('../opt/ml/input/data/test/test.csv', index=False)"
   ]
  },
  {
   "source": [
    "## 1-3.学習"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM が扱うデータセットの形式に直す\n",
    "dtrain = lgb.Dataset(tr_x, label=tr_y)\n",
    "dvalid = lgb.Dataset(va_x, label=va_y)\n",
    "dtest = lgb.Dataset(test_x)\n",
    "\n",
    "# 学習用のパラメータ\n",
    "lgb_params = {\n",
    "    # 二値分類問題\n",
    "    'objective': 'binary',\n",
    "    # 評価指標\n",
    "    'metrics': 'binary_logloss',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 237, number of negative: 142\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3798\n",
      "[LightGBM] [Info] Number of data points in the train set: 379, number of used features: 30\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625330 -> initscore=0.512233\n",
      "[LightGBM] [Info] Start training from score 0.512233\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\ttrain's binary_logloss: 0.583245\tvalid's binary_logloss: 0.582221\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\ttrain's binary_logloss: 0.518624\tvalid's binary_logloss: 0.519402\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\ttrain's binary_logloss: 0.467531\tvalid's binary_logloss: 0.471914\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\ttrain's binary_logloss: 0.423087\tvalid's binary_logloss: 0.426939\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\ttrain's binary_logloss: 0.385211\tvalid's binary_logloss: 0.391658\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\ttrain's binary_logloss: 0.349443\tvalid's binary_logloss: 0.356531\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\ttrain's binary_logloss: 0.317067\tvalid's binary_logloss: 0.327033\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\ttrain's binary_logloss: 0.289598\tvalid's binary_logloss: 0.299828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\ttrain's binary_logloss: 0.267131\tvalid's binary_logloss: 0.278931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\ttrain's binary_logloss: 0.245916\tvalid's binary_logloss: 0.258254\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\ttrain's binary_logloss: 0.226934\tvalid's binary_logloss: 0.241703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\ttrain's binary_logloss: 0.210347\tvalid's binary_logloss: 0.22565\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\ttrain's binary_logloss: 0.19611\tvalid's binary_logloss: 0.213778\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\ttrain's binary_logloss: 0.181945\tvalid's binary_logloss: 0.197933\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\ttrain's binary_logloss: 0.168331\tvalid's binary_logloss: 0.18491\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\ttrain's binary_logloss: 0.157474\tvalid's binary_logloss: 0.176478\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\ttrain's binary_logloss: 0.145925\tvalid's binary_logloss: 0.167553\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\ttrain's binary_logloss: 0.135624\tvalid's binary_logloss: 0.157974\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\ttrain's binary_logloss: 0.126928\tvalid's binary_logloss: 0.151323\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\ttrain's binary_logloss: 0.11911\tvalid's binary_logloss: 0.144895\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\ttrain's binary_logloss: 0.111022\tvalid's binary_logloss: 0.135649\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\ttrain's binary_logloss: 0.103684\tvalid's binary_logloss: 0.12993\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\ttrain's binary_logloss: 0.0962253\tvalid's binary_logloss: 0.123531\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\ttrain's binary_logloss: 0.0896759\tvalid's binary_logloss: 0.117425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\ttrain's binary_logloss: 0.0838193\tvalid's binary_logloss: 0.111944\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\ttrain's binary_logloss: 0.0785658\tvalid's binary_logloss: 0.107157\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\ttrain's binary_logloss: 0.0736935\tvalid's binary_logloss: 0.101922\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\ttrain's binary_logloss: 0.0681601\tvalid's binary_logloss: 0.100178\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\ttrain's binary_logloss: 0.0634425\tvalid's binary_logloss: 0.0957191\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\ttrain's binary_logloss: 0.0592376\tvalid's binary_logloss: 0.0930103\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\ttrain's binary_logloss: 0.0554449\tvalid's binary_logloss: 0.0896791\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\ttrain's binary_logloss: 0.0510206\tvalid's binary_logloss: 0.0879681\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\ttrain's binary_logloss: 0.0471317\tvalid's binary_logloss: 0.0868479\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\ttrain's binary_logloss: 0.0440876\tvalid's binary_logloss: 0.0822513\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\ttrain's binary_logloss: 0.0407371\tvalid's binary_logloss: 0.0805727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\ttrain's binary_logloss: 0.03793\tvalid's binary_logloss: 0.0788549\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\ttrain's binary_logloss: 0.0349672\tvalid's binary_logloss: 0.0761908\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\ttrain's binary_logloss: 0.0326998\tvalid's binary_logloss: 0.0738255\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\ttrain's binary_logloss: 0.0306578\tvalid's binary_logloss: 0.0698516\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\ttrain's binary_logloss: 0.0283008\tvalid's binary_logloss: 0.0689386\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\ttrain's binary_logloss: 0.0263106\tvalid's binary_logloss: 0.0671285\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\ttrain's binary_logloss: 0.024677\tvalid's binary_logloss: 0.0644577\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\ttrain's binary_logloss: 0.0229245\tvalid's binary_logloss: 0.0611481\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\ttrain's binary_logloss: 0.0211353\tvalid's binary_logloss: 0.0579387\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\ttrain's binary_logloss: 0.019486\tvalid's binary_logloss: 0.0570836\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\ttrain's binary_logloss: 0.0179818\tvalid's binary_logloss: 0.0566918\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\ttrain's binary_logloss: 0.0170773\tvalid's binary_logloss: 0.0560871\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\ttrain's binary_logloss: 0.0157177\tvalid's binary_logloss: 0.0534724\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\ttrain's binary_logloss: 0.0147924\tvalid's binary_logloss: 0.051641\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\ttrain's binary_logloss: 0.013864\tvalid's binary_logloss: 0.050688\n"
     ]
    }
   ],
   "source": [
    "# モデルを学習する\n",
    "# バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n",
    "# watchlistには学習データおよびバリデーションデータをセットする\n",
    "#watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "model = lgb.train(lgb_params,\n",
    "                dtrain,\n",
    "                num_boost_round=50,  # 学習ラウンド数は適当\n",
    "                #evals=watchlist\n",
    "                valid_names=['train','valid'], valid_sets=[dtrain, dvalid]\n",
    "                )"
   ]
  },
  {
   "source": [
    "## 1-4. 予測・評価"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# 予測：検証用データが各クラスに分類される確率を計算する\n",
    "pred_proba = model.predict(test_x)\n",
    "# しきい値 0.5 で 0, 1 に丸める\n",
    "pred = np.where(pred_proba > 0.5, 1, 0)\n",
    "# 精度 (Accuracy) を検証する\n",
    "acc = accuracy_score(test_y, pred)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "source": [
    "## 2.ローカル環境で学習ジョブ風に実行する（コンテナ未使用）\n",
    "コンテナを導入する前に、ローカル環境でSageMaker学習ジョブのように動かします。.pyファイルの動作確認を高速で行うことが目的です。\n",
    "ディレクトリ構造"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2-1. データ配置"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([tr_x,tr_y], axis=1).to_csv('../opt/ml/input/data/train/train.csv', index=False)\n",
    "pd.concat([va_x,va_y], axis=1).to_csv('../opt/ml/input/data/valid/valid.csv', index=False)\n",
    "pd.concat([test_x,test_y], axis=1).to_csv('../opt/ml/input/data/test/test.csv', index=False)"
   ]
  },
  {
   "source": [
    "## 2-2. ソースコードを準備\n",
    "../opt/ml/input/data/src/train.pyに配置"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ../opt/program/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ../opt/ml/input/data/src/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[37m#!/usr/bin/env python\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlightgbm\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mlgb\u001b[39;49;00m\n\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.metrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score\n\n\u001b[37m# データ読み込み\u001b[39;49;00m\ntrain_df = pd.read_csv(\u001b[33m'\u001b[39;49;00m\u001b[33m../opt/ml/input/data/train/train.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\nvalid_df = pd.read_csv(\u001b[33m'\u001b[39;49;00m\u001b[33m../opt/ml/input/data/valid/valid.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\ntest_df = pd.read_csv(\u001b[33m'\u001b[39;49;00m\u001b[33m../opt/ml/input/data/test/test.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n\ntr_x, tr_y = train_df.drop([\u001b[33m'\u001b[39;49;00m\u001b[33mtarget\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m), train_df[\u001b[33m'\u001b[39;49;00m\u001b[33mtarget\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\nva_x, va_y = valid_df.drop([\u001b[33m'\u001b[39;49;00m\u001b[33mtarget\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m), valid_df[\u001b[33m'\u001b[39;49;00m\u001b[33mtarget\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\ntest_x, test_y = test_df.drop([\u001b[33m'\u001b[39;49;00m\u001b[33mtarget\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m), test_df[\u001b[33m'\u001b[39;49;00m\u001b[33mtarget\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n\n\u001b[37m# 学習準備\u001b[39;49;00m\n\u001b[37m# LightGBM が扱うデータセットの形式に直す\u001b[39;49;00m\ndtrain = lgb.Dataset(tr_x, label=tr_y)\ndvalid = lgb.Dataset(va_x, label=va_y)\ndtest = lgb.Dataset(test_x)\n\n\u001b[37m# 学習用のパラメータ\u001b[39;49;00m\nlgb_params = {\n    \u001b[37m# 二値分類問題\u001b[39;49;00m\n    \u001b[33m'\u001b[39;49;00m\u001b[33mobjective\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mbinary\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n    \u001b[37m# 評価指標\u001b[39;49;00m\n    \u001b[33m'\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mbinary_logloss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n}\n\n\u001b[37m# 学習\u001b[39;49;00m\n\u001b[37m# モデルを学習する\u001b[39;49;00m\n\u001b[37m# バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\u001b[39;49;00m\n\u001b[37m# watchlistには学習データおよびバリデーションデータをセットする\u001b[39;49;00m\n\u001b[37m#watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\u001b[39;49;00m\nmodel = lgb.train(lgb_params,\n                dtrain,\n                num_boost_round=\u001b[34m50\u001b[39;49;00m,  \u001b[37m# 学習ラウンド数は適当\u001b[39;49;00m\n                \u001b[37m#evals=watchlist\u001b[39;49;00m\n                valid_names=[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mvalid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], valid_sets=[dtrain, dvalid]\n                )\n\u001b[37m# 予測\u001b[39;49;00m\n\u001b[37m# 予測：検証用データが各クラスに分類される確率を計算する\u001b[39;49;00m\npred_proba = model.predict(test_x)\n\u001b[37m# しきい値 0.5 で 0, 1 に丸める\u001b[39;49;00m\npred = np.where(pred_proba > \u001b[34m0.5\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m)\n\u001b[37m# 評価\u001b[39;49;00m\n\u001b[37m# 精度 (Accuracy) を検証する\u001b[39;49;00m\nacc = accuracy_score(test_y, pred)\n\u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mAccuracy:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, acc)\n\u001b[37m# 予測結果を出力\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ../opt/ml/input/data/src/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train running...\n",
      "/Users/yshiy/github/sagemaker-byoc/notebook\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 237, number of negative: 142\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3798\n",
      "[LightGBM] [Info] Number of data points in the train set: 379, number of used features: 30\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625330 -> initscore=0.512233\n",
      "[LightGBM] [Info] Start training from score 0.512233\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\ttrain's binary_logloss: 0.583245\tvalid's binary_logloss: 0.582221\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\ttrain's binary_logloss: 0.518624\tvalid's binary_logloss: 0.519402\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\ttrain's binary_logloss: 0.467531\tvalid's binary_logloss: 0.471914\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\ttrain's binary_logloss: 0.423087\tvalid's binary_logloss: 0.426939\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\ttrain's binary_logloss: 0.385211\tvalid's binary_logloss: 0.391658\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\ttrain's binary_logloss: 0.349443\tvalid's binary_logloss: 0.356531\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\ttrain's binary_logloss: 0.317067\tvalid's binary_logloss: 0.327033\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\ttrain's binary_logloss: 0.289598\tvalid's binary_logloss: 0.299828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\ttrain's binary_logloss: 0.267131\tvalid's binary_logloss: 0.278931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\ttrain's binary_logloss: 0.245916\tvalid's binary_logloss: 0.258254\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\ttrain's binary_logloss: 0.226934\tvalid's binary_logloss: 0.241703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\ttrain's binary_logloss: 0.210347\tvalid's binary_logloss: 0.22565\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\ttrain's binary_logloss: 0.19611\tvalid's binary_logloss: 0.213778\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\ttrain's binary_logloss: 0.181945\tvalid's binary_logloss: 0.197933\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\ttrain's binary_logloss: 0.168331\tvalid's binary_logloss: 0.18491\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\ttrain's binary_logloss: 0.157474\tvalid's binary_logloss: 0.176478\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\ttrain's binary_logloss: 0.145925\tvalid's binary_logloss: 0.167553\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\ttrain's binary_logloss: 0.135624\tvalid's binary_logloss: 0.157974\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\ttrain's binary_logloss: 0.126928\tvalid's binary_logloss: 0.151323\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\ttrain's binary_logloss: 0.11911\tvalid's binary_logloss: 0.144895\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\ttrain's binary_logloss: 0.111022\tvalid's binary_logloss: 0.135649\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\ttrain's binary_logloss: 0.103684\tvalid's binary_logloss: 0.12993\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\ttrain's binary_logloss: 0.0962253\tvalid's binary_logloss: 0.123531\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\ttrain's binary_logloss: 0.0896759\tvalid's binary_logloss: 0.117425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\ttrain's binary_logloss: 0.0838193\tvalid's binary_logloss: 0.111944\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\ttrain's binary_logloss: 0.0785658\tvalid's binary_logloss: 0.107157\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\ttrain's binary_logloss: 0.0736935\tvalid's binary_logloss: 0.101922\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\ttrain's binary_logloss: 0.0681601\tvalid's binary_logloss: 0.100178\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\ttrain's binary_logloss: 0.0634425\tvalid's binary_logloss: 0.0957191\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\ttrain's binary_logloss: 0.0592376\tvalid's binary_logloss: 0.0930103\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\ttrain's binary_logloss: 0.0554449\tvalid's binary_logloss: 0.0896791\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\ttrain's binary_logloss: 0.0510206\tvalid's binary_logloss: 0.0879681\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\ttrain's binary_logloss: 0.0471317\tvalid's binary_logloss: 0.0868479\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\ttrain's binary_logloss: 0.0440876\tvalid's binary_logloss: 0.0822513\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\ttrain's binary_logloss: 0.0407371\tvalid's binary_logloss: 0.0805727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\ttrain's binary_logloss: 0.03793\tvalid's binary_logloss: 0.0788549\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\ttrain's binary_logloss: 0.0349672\tvalid's binary_logloss: 0.0761908\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\ttrain's binary_logloss: 0.0326998\tvalid's binary_logloss: 0.0738255\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\ttrain's binary_logloss: 0.0306578\tvalid's binary_logloss: 0.0698516\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\ttrain's binary_logloss: 0.0283008\tvalid's binary_logloss: 0.0689386\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\ttrain's binary_logloss: 0.0263106\tvalid's binary_logloss: 0.0671285\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\ttrain's binary_logloss: 0.024677\tvalid's binary_logloss: 0.0644577\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\ttrain's binary_logloss: 0.0229245\tvalid's binary_logloss: 0.0611481\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\ttrain's binary_logloss: 0.0211353\tvalid's binary_logloss: 0.0579387\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\ttrain's binary_logloss: 0.019486\tvalid's binary_logloss: 0.0570836\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\ttrain's binary_logloss: 0.0179818\tvalid's binary_logloss: 0.0566918\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\ttrain's binary_logloss: 0.0170773\tvalid's binary_logloss: 0.0560871\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\ttrain's binary_logloss: 0.0157177\tvalid's binary_logloss: 0.0534724\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\ttrain's binary_logloss: 0.0147924\tvalid's binary_logloss: 0.051641\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\ttrain's binary_logloss: 0.013864\tvalid's binary_logloss: 0.050688\n",
      "Accuracy: 0.9473684210526315\n",
      "train end\n"
     ]
    }
   ],
   "source": [
    "!../opt/program/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# 3.ローカルモードで学習ジョブを実行"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### ---------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.コンテナ準備\n",
    "## 1-1.Dockerfile (確認のみ)\n",
    "\n",
    "Dockerfileには、構築したいイメージが記述されています。これは、実行したいシステムの完全なオペレーティングシステムのインストールを記述していると考えることができます。しかし、Dockerコンテナの実行は、基本的な操作のためにホストマシン上のLinuxを利用するため、完全なオペレーティングシステムよりもかなり軽量です。\n",
    "\n",
    "Pythonサイエンススタックでは、標準的なUbuntuのインストールから始めて、通常のツールを実行してscikit-learnで必要なものをインストールします。最後に、特定のアルゴリズムを実装したコードをコンテナに追加して、実行に適した環境を整えます。\n",
    "\n",
    "その際、余分なスペースを整理します。これにより、コンテナは小さくなり、起動も速くなります。\n",
    "\n",
    "例のDockerfileを見てみましょう。"
   ]
  },
  {
   "source": [
    "imageには、trainやbacktestに必要なソースは含めないこととする。\n",
    "（学習ジョブ実行時にS3からコピーする）\n",
    "よって、dockerイメージ作成時に必要な資材はない。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FROM python:3.7.5-slim\nUSER root\n\nRUN apt-get update\nRUN apt-get -y install locales && \\\n    localedef -f UTF-8 -i ja_JP ja_JP.UTF-8\nENV LANG ja_JP.UTF-8\nENV LANGUAGE ja_JP:ja\nENV LC_ALL ja_JP.UTF-8\nENV TZ JST-9\nENV TERM xterm\n\nRUN apt-get install -y vim less\nRUN pip install --upgrade pip\nRUN pip install --upgrade setuptools\n\nRUN apt-get -y install build-essential\nRUN apt-get -y install wget\n\n### install libraries\nRUN pip install numpy pandas scikit-learn matplotlib seaborn lightgbm\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"/opt/program:${PATH}\"\n\n# Set up the program in the image\nCOPY program /opt/program\nWORKDIR /opt/program"
     ]
    }
   ],
   "source": [
    "!cat ../container/lgbm/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2.Building and registering the container to ECR\n",
    "\n",
    "以下のシェルコードは、`docker build`を使用してコンテナイメージをビルドし、`docker push`を使用してコンテナイメージをECRにプッシュする方法を示しています。このコードはシェルスクリプト `container/build-and-push.sh` としても提供されており、`build-and-push.sh decision_trees_sample` として実行することで、イメージ `decision_trees_sample` をビルドすることができます。\n",
    "\n",
    "このコードは、使用しているアカウントと現在のデフォルトリージョン（SageMakerのノートブックインスタンスを使用している場合は、ノートブックインスタンスが作成されたリージョンになります）でECRリポジトリを探します。\n",
    "リポジトリが存在しない場合、スクリプトはそれを作成します。"
   ]
  },
  {
   "source": [
    "https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/container/build_and_push.sh\n",
    "\n",
    "以下は、build-and-push.shと同じ内容"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Login Succeeded\n",
      "The push refers to repository [805433377179.dkr.ecr.us-east-1.amazonaws.com/test-trainingjob]\n",
      "sh: line 5: cd: container/lgbm: No such file or directory\n",
      "chmod: program/train: No such file or directory\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 sha256:d18e60c53e7e59f8b43609d7e6ff720d01c6520e4a3834900c4eb655bdd90dc1\n",
      "#1 transferring dockerfile: 2B 0.0s done\n",
      "#1 DONE 0.0s\n",
      "failed to solve with frontend dockerfile.v0: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount750953024/Dockerfile: no such file or directory\n",
      "Error response from daemon: No such image: test-trainingjob:latest\n",
      "An image does not exist locally with the tag: 805433377179.dkr.ecr.us-east-1.amazonaws.com/test-trainingjob\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\n# The name of our algorithm\\nalgorithm_name=test-trainingjob\\n\\ncd container/lgbm\\n\\nchmod +x program/train\\n#chmod +x decision_trees/serve\\n\\naccount=$(aws sts get-caller-identity --query Account --output text)\\n\\n# Get the region defined in the current configuration (default to us-west-2 if none defined)\\nregion=$(aws configure get region)\\nregion=${region:-us-west-1}\\n\\nfullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\\n\\n# If the repository doesn\\'t exist in ECR, create it.\\naws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\\n\\nif [ $? -ne 0 ]\\nthen\\n    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\\nfi\\n\\n# Get the login command from ECR and execute it directly\\naws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\\n\\n# Build the docker image locally with the image name and then push it to ECR\\n# with the full name.\\n\\ndocker build  -t ${algorithm_name} .\\ndocker tag ${algorithm_name} ${fullname}\\n\\ndocker push ${fullname}\\n'' returned non-zero exit status 1.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8a7e40c619dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n# The name of our algorithm\\nalgorithm_name=test-trainingjob\\n\\ncd container/lgbm\\n\\nchmod +x program/train\\n#chmod +x decision_trees/serve\\n\\naccount=$(aws sts get-caller-identity --query Account --output text)\\n\\n# Get the region defined in the current configuration (default to us-west-2 if none defined)\\nregion=$(aws configure get region)\\nregion=${region:-us-west-1}\\n\\nfullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\\n\\n# If the repository doesn\\'t exist in ECR, create it.\\naws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\\n\\nif [ $? -ne 0 ]\\nthen\\n    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\\nfi\\n\\n# Get the login command from ECR and execute it directly\\naws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\\n\\n# Build the docker image locally with the image name and then push it to ECR\\n# with the full name.\\n\\ndocker build  -t ${algorithm_name} .\\ndocker tag ${algorithm_name} ${fullname}\\n\\ndocker push ${fullname}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</opt/anaconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\n# The name of our algorithm\\nalgorithm_name=test-trainingjob\\n\\ncd container/lgbm\\n\\nchmod +x program/train\\n#chmod +x decision_trees/serve\\n\\naccount=$(aws sts get-caller-identity --query Account --output text)\\n\\n# Get the region defined in the current configuration (default to us-west-2 if none defined)\\nregion=$(aws configure get region)\\nregion=${region:-us-west-1}\\n\\nfullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\\n\\n# If the repository doesn\\'t exist in ECR, create it.\\naws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\\n\\nif [ $? -ne 0 ]\\nthen\\n    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\\nfi\\n\\n# Get the login command from ECR and execute it directly\\naws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\\n\\n# Build the docker image locally with the image name and then push it to ECR\\n# with the full name.\\n\\ndocker build  -t ${algorithm_name} .\\ndocker tag ${algorithm_name} ${fullname}\\n\\ndocker push ${fullname}\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=test-trainingjob\n",
    "\n",
    "cd ../container/lgbm\n",
    "\n",
    "chmod +x program/train\n",
    "#chmod +x decision_trees/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "source": [
    "## 疑問：build and push したイメージの動作確認はローカルでできる？\n",
    "できる。ローカルモード"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# -----コンテナ準備完了-----"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. データとプログラムをS3にアップロード\n",
    "・SageMaker　SDKを使う場合。  \n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/api/utility/session.html\n",
    "\n",
    "\n",
    "・boto3でs3クライアントを使う場合。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "sess = sage.Session()\n",
    "\n",
    "# S3 prefix\n",
    "#prefix = 'DEMO-scikit-byo-iris'\n",
    "prefix = 'test-rd/src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORK_DIRECTORY = 'data'\n",
    "#data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n",
    "\n",
    "src_location = sess.upload_data('opt/ml/input/data/src', bucket='work-aws-virginia', key_prefix=prefix)"
   ]
  },
  {
   "source": [
    "# -----データ準備完了-----"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-1.学習ジョブ発行(ローカルモード)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_location = 's3://work-aws-virginia/test-rd/candles/'\n",
    "data_location = 's3://work-aws-virginia/test-rd/train/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "#image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-decision-trees:latest'.format(account, region)\n",
    "\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/test-rd:latest'.format(account, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={'candle_window': 15,\n",
    "                 'horizon': 300,\n",
    "                 'target': 'tgt_diff'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = 'arn:aws:iam::805433377179:role/sagemaker-sdk-for-local'\n",
    "\n",
    "lgbm = sage.estimator.Estimator(image_uri=image,\n",
    "                                #entasdfdfry_point1='run.sh',\n",
    "                                #source_sddddddddddir='src',\n",
    "                                role=role, \n",
    "                                instance_count=1,\n",
    "                                instance_type='local',\n",
    "                                #instance_type='ml.c4.2xlarge',\n",
    "                                output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                                hyperparameters=hyperparameters,\n",
    "                                #sagemaker_session=sess)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.fit({'train':'s3://work-aws-virginia/test-rd/train/',\n",
    "          'valid':'s3://work-aws-virginia/test-rd/train/',\n",
    "          'test' :'s3://work-aws-virginia/test-rd/train/',\n",
    "          'backtest':'s3://work-aws-virginia/test-rd/train/',\n",
    "          'src':'s3://work-aws-virginia/test-rd/src/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# 3-2.学習ジョブ発行(SageMaker)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 下準備"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "sess = sage.Session()\n",
    "\n",
    "prefix = 'test-rd/src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'timeframe': 15, 'horizon': 300, 'hist_row': 2}\n"
     ]
    }
   ],
   "source": [
    "### ソースコードをアップロード\n",
    "src_location = sess.upload_data('opt/ml/input/data/src', bucket='work-aws-virginia', key_prefix=prefix)\n",
    "\n",
    "### ハイパーパラメータ設定（ローカルファイルから読み込み）\n",
    "hyperparameters={\"timeframe\": 15,\n",
    "                 \"horizon\" : 300,\n",
    "                 \"hist_row\" : 2\n",
    "}\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'timeframe': 15, 'horizon': 300, 'hist_row': 2, 'target_col': 'tgt_diff'}\n"
     ]
    }
   ],
   "source": [
    "HYPERPARAMETER_JSON_PATH = \"../../../../../opt/ml/input/config/hyperparameters.json\"\n",
    "HYPERPARAMETER_JSON_PATH = \"opt/ml/input/config/hyperparameters.json\"\n",
    "\n",
    "import json\n",
    "### ハイパーパラメータ設定（ローカルファイルから読み込み）\n",
    "with open(HYPERPARAMETER_JSON_PATH, \"r\") as f:\n",
    "    hyperparameters = json.load(f)\n",
    "\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = 'arn:aws:iam::805433377179:role/sagemaker-sdk-for-local'\n",
    "#role = 'arn:aws:iam::805433377179:role/service-role/AmazonSageMaker-ExecutionRole-20191212T111531'\n",
    "\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/test-rd:latest'.format(account, region)\n",
    "\n",
    "lgbm = sage.estimator.Estimator(image_uri=image,\n",
    "                                #entasdfdfry_point1='run.sh',\n",
    "                                #source_sddddddddddir='src',\n",
    "                                role=role, \n",
    "                                instance_count=1,\n",
    "                                #instance_type='local',\n",
    "                                instance_type='ml.c4.2xlarge',\n",
    "                                #output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                                output_path='s3://work-aws-virginia/test-rd/output/',\n",
    "                                #sagemaker_session=sess # ノートブックインスタンスで実行する場合にIAMを渡す\n",
    "                                hyperparameters=hyperparameters,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.fit({'train':'s3://work-aws-virginia/test-rd/train/',\n",
    "          'valid':'s3://work-aws-virginia/test-rd/valid/',\n",
    "          'test' :'s3://work-aws-virginia/test-rd/test/',\n",
    "          #'backtest':'s3://work-aws-virginia/test-rd/test/',\n",
    "          'src':'s3://work-aws-virginia/test-rd/src/'},\n",
    "          wait=False\n",
    ")"
   ]
  },
  {
   "source": [
    "\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase.fit\n",
    "\n",
    "・並行してジョブ発行する場合は、wait=Falseを使う"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 並列実行"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "sess = sage.Session()\n",
    "\n",
    "prefix = 'test-rd/src'\n",
    "### ソースコードをアップロード\n",
    "src_location = sess.upload_data('opt/ml/input/data/src', bucket='work-aws-virginia', key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_list = [\n",
    "    'opt/ml/input/config/hyperparameters_10_180_2_diff.json',\n",
    "    'opt/ml/input/config/hyperparameters_10_300_2_diff.json',\n",
    "    'opt/ml/input/config/hyperparameters_15_180_2_diff.json',\n",
    "    'opt/ml/input/config/hyperparameters_15_300_2_diff.json',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "opt/ml/input/config/hyperparameters_10_180_2_diff.json\n",
      "{'timeframe': 10, 'horizon': 180, 'hist_row': 2, 'target_col': 'tgt_diff'}\n",
      "opt/ml/input/config/hyperparameters_10_300_2_diff.json\n",
      "{'timeframe': 10, 'horizon': 300, 'hist_row': 2, 'target_col': 'tgt_diff'}\n",
      "opt/ml/input/config/hyperparameters_15_180_2_diff.json\n",
      "{'timeframe': 15, 'horizon': 180, 'hist_row': 2, 'target_col': 'tgt_diff'}\n",
      "opt/ml/input/config/hyperparameters_15_300_2_diff.json\n",
      "{'timeframe': 15, 'horizon': 300, 'hist_row': 2, 'target_col': 'tgt_diff'}\n"
     ]
    }
   ],
   "source": [
    "role = 'arn:aws:iam::805433377179:role/sagemaker-sdk-for-local'\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/test-rd:latest'.format(account, region)\n",
    "\n",
    "import json\n",
    "for hp_path in hyperparam_list:\n",
    "    print(hp_path)\n",
    "\n",
    "    ### ハイパーパラメータ設定（ローカルファイルから読み込み）\n",
    "    with open(hp_path, \"r\") as f:\n",
    "        hyperparameters = json.load(f)\n",
    "\n",
    "    print(hyperparameters)\n",
    "\n",
    "    lgbm = sage.estimator.Estimator(image_uri=image,\n",
    "                                role=role, \n",
    "                                instance_count=1,\n",
    "                                instance_type='ml.c4.2xlarge',\n",
    "                                output_path='s3://work-aws-virginia/test-rd/output/',\n",
    "                                hyperparameters=hyperparameters,\n",
    "                                )\n",
    "    lgbm.fit({'train':'s3://work-aws-virginia/test-rd/train/',\n",
    "          'valid':'s3://work-aws-virginia/test-rd/valid/',\n",
    "          'test' :'s3://work-aws-virginia/test-rd/test/',\n",
    "          'src':'s3://work-aws-virginia/test-rd/src/'},\n",
    "          wait=False\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# 上記のEstimatorを編集していく\n",
    "https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#\n",
    "\n",
    "classsagemaker.estimator.Estimator(image_uri, role, instance_count=None, instance_type=None, volume_size=30, volume_kms_key=None, max_run=86400, input_mode='File', output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, hyperparameters=None, tags=None, subnets=None, security_group_ids=None, model_uri=None, model_channel_name='model', metric_definitions=None, encrypt_inter_container_traffic=False, use_spot_instances=False, max_wait=None, checkpoint_s3_uri=None, checkpoint_local_path=None, enable_network_isolation=False, rules=None, debugger_hook_config=None, tensorboard_output_config=None, enable_sagemaker_metrics=None, profiler_config=None, disable_profiler=False, environment=None, **kwargs)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker==2.33.0"
   ]
  },
  {
   "source": [
    "SageMaker Python SDK version == 2.33.0　でOK\n",
    "\n",
    "\n",
    "\n",
    "SageMaker Python SDKのローカルモードを利用して、ノートブックインスタンス以外の環境で学習ジョブを回してみる\n",
    "https://dev.classmethod.jp/articles/sagemaker-python-sdk-localmode/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python374jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}